#+TITLE: Process preliminary RAMA data
#+AUTHOR: Deepak Cherian
#+OPTIONS: timestamp:nil title:t html5-fancy:t html-style:nil html-scripts:nil

#+LATEX_CLASS: dcnotebook

#+HTML_HEAD: <link rel="stylesheet" href="notebook.css" type="text/css" />
* Info/emails
** Quality Codes:

  0 = datum missing

  1 = highest quality; Pre/post-deployment calibrations agree to within
  sensor specifications.  In most cases only pre-deployment calibrations
  have been applied

  2 = default quality; Pre-deployment calibrations applied.  Default
  value for sensors presently deployed and for sensors which were either
  not recovered or not calibratable when recovered.

  3 = adjusted data; Pre/post calibrations differ, or original data do
  not agree with other data sources (e.g., other in situ data or
  climatology), or original data are noisy.  Data have been adjusted in
  an attempt to reduce the error.

  4 = lower quality; Pre/post calibrations differ, or data do not agree
  with other data sources (e.g., other in situ data or climatology), or
  data are noisy.  Data could not be confidently adjusted to correct
  for error.

  5 = sensor or tube failed

** filtering salinity
From https://www.pmel.noaa.gov/gtmba/sampling
#+BEGIN_QUOTE
To minimize spiking in the salinity record due to sensor response time mismatches, the internally recorded 10-minute conductivity and temperature values are smoothed with a 13-point Hanning filter and subsampled at hourly intervals. Salinities are calculated from the smoothed hourly conductivity and temperature values using the method of Fofonoff and Millard (1983). These hourly data constitute the high-resolution salinity time series in the data base. High-resolution temperatures are offered at their original 10-minute sampling increment.
#+END_QUOTE
** March 27, 2017 Sonya
#+BEGIN_QUOTE

Hi Deepak,

I've placed two sets of pre-calibrated subsurface data into ftp://ftp.pmel.noaa.gov/OCRD/tao/ForDeepak/, as well as the post-calibrated salinity files from ra107 *(post cals are not available yet for the ra122 data)*. The ~.flg files are text files containing the data (with the pre or post-calibrations applied). The ~sum files are log files listing the instruments in each mooring along with some notes about them (we add more notes to these as we process the mooring). In the "postcal" directory you'll find, alongside the post-calibrated ~flg files, *two files containing the pre-cal and post-cal coefficients, in case you're curious (since they've already been applied, you most likely won't need them).* Note that in typical salinity processing,
1. [X] I would first flag data that appeared bad and unfixable,
2. [X] and then I'd apply a pre-to-post-cal linear interpolation across the data to correct for large-scale drifts (i.e. the first data value in the interpolated output would have a pre-cal applied, while the last value would have a post-cal applied, with the values interpolated in between).
3. [X] After that I'd compare the end points with previous and subsequent moorings at the same site, as well as any available CTDs,
4. [ ] and I'd check inter-depth differences for density inversions.
5. [ ] Using these three pieces of information, I'd manually adjust the data as needed (sometimes with dozens of adjustments needed to eliminate large-scale density inversions and bring various depths back into the mixed layer) before filtering them (into hourly data) and releasing them.

I hope this helps. Take care and please let me know if you need anything else, -Sonya

p.s. The 10m current data will be posted later, - I'll let you know when those are posted as well....

#+END_QUOTE
* Functions           :noexport:

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results
  import numpy as np
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  import datetime as dt
  import cmocean as cmo
  import seawater as sw
  from copy import copy

  mpl.rcParams['savefig.transparent'] = True
  mpl.rcParams['figure.figsize'] = [6.5, 6.5]
  mpl.rcParams['figure.dpi'] = 180
  mpl.rcParams['axes.facecolor'] = 'None'

  # ra107['sal][ra107['sal'] > 40] = np.NaN

  def smooth(x,window_len=11,window='hanning'):
      """smooth the data using a window with requested size.

      This method is based on the convolution of a scaled window with the signal.
      The signal is prepared by introducing reflected copies of the signal
      (with the window size) in both ends so that transient parts are minimized
      in the begining and end part of the output signal.

      input:
          x: the input signal
          window_len: the dimension of the smoothing window; should be an odd integer
          window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'
              flat window will produce a moving average smoothing.

      output:
          the smoothed signal

      example:

      t=linspace(-2,2,0.1)
      x=sin(t)+randn(len(t))*0.1
      y=smooth(x)

      see also:

      numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
      scipy.signal.lfilter

      TODO: the window parameter could be the window itself if an array instead of a string
      NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.
      """

      if x.ndim != 1:
          raise ValueError("smooth only accepts 1 dimension arrays.")

      if x.size < window_len:
          raise ValueError("Input vector needs to be bigger than window size.")


      if window_len<3:
          return x


      if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:
          raise ValueError("Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'")

      s=np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]
      #print(len(s))
      if window == 'flat': #moving average
          w=np.ones(window_len,'d')
      else:
          w=eval('np.'+window+'(window_len)')

      y=np.convolve(w/w.sum(), s, mode='valid')
      return y[(window_len/2-1):-(window_len/2+1)]


  def CleanSalinity(salinity):
    """ Adds NaNs in place of missing values. """
    import numpy as np

    salinity = np.float32(salinity)

    if salinity > 39:
      salinity = np.nan

    return salinity

  def ProcessDate(datestr):
    """ Takes in string of form YYYYydayHHMM and returns python datetime object."""
    import datetime as dt

    year = int(datestr[0:4])
    yday = int(datestr[4:7])
    hour = int(datestr[7:9])
    mins = int(datestr[9:11])

    date = dt.datetime(year=year, month=1, day=1) \
			     +  dt.timedelta(days=yday-1, hours=hour, minutes=mins)

    return date

  sal = np.dtype([('date', dt.datetime),
		  ('sal', [('1', np.float32),
                           ('10', np.float32),
                           ('20', np.float32),
                           ('40', np.float32),
                           ('60', np.float32),
                           ('100', np.float32)]),
		  ('QQQQQQ', np.uint32),
		  ('SSSSSS', np.uint32)])

  temp = np.dtype([('date', dt.datetime),
		   ('temp', [('1', np.float32),
                             ('10', np.float32),
                             ('13', np.float32),
                             ('20', np.float32),
                             ('40', np.float32),
                             ('60', np.float32),
                             ('80', np.float32),
                             ('100', np.float32),
                             ('120', np.float32),
                             ('140', np.float32),
                             ('180', np.float32),
                             ('300', np.float32),
                             ('500', np.float32)]),
		   ('QQQQQQ', np.uint32),
		   ('SSSSSS', np.uint32)])

  dens = np.dtype([('date', dt.datetime),
		   ('dens', [('1', np.float32),
                            ('10', np.float32),
                            ('20', np.float32),
                            ('40', np.float32),
                            ('60', np.float32),
                            ('100', np.float32)]),
		   ('QQQQQQ', np.uint32),
		   ('SSSSSS', np.uint32)])
#+END_SRC

#+RESULTS:

* RAMA13 (ra-107)
** Read data
From Sonya:
#+BEGIN_quote
  I'd apply a /pre-to-post-cal linear interpolation/ across the data to correct for large-scale drifts (i.e. the first data value in the interpolated output would have a pre-cal applied, while the last value would have a post-cal applied, with the values interpolated in between).
#+END_QUOTE

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results

  # setup a mooring dictionary
  ra107 = dict([])
  ra107['name'] = 'RAMA13'
  ra107['date'] = ra107pre['date']
  ra107['sal']  = dict([])
  ra107['temp'] = dict([])
  ra107['dens'] = dict([])
  ra107['sal-hr'] = dict([])
  ra107['temp-hr'] = dict([])
  ra107['dens-hr'] = dict([])

  cnv = {0:ProcessDate}
  for jj in np.arange(1,7):
      cnv[jj] = CleanSalinity;

  ra107pre = np.loadtxt('../TAO_raw/sal107a.flg', skiprows=5, dtype=sal,
			converters=cnv)
  ra107['sal-pre'] = ra107pre['sal']
  ra107post = np.loadtxt('../TAO_raw/postcal/sal107a.flg', skiprows=5,
			 dtype=sal, converters=cnv)
  ra107['sal-post'] = ra107post['sal']

  ra107pre = np.loadtxt('../TAO_raw/dens107a.flg', skiprows=5,
			dtype=dens, converters=cnv)
  ra107['dens-pre'] = ra107pre['dens']

  ra107post = np.loadtxt('../TAO_raw/postcal/dens107a.flg', skiprows=5,
			 dtype=dens, converters=cnv)
  ra107['dens-post'] = ra107post['dens']

  # now for pre-calib temperature
  cnv = {0:ProcessDate}
  for jj in np.arange(1,14):
      cnv[jj] = CleanSalinity;
  ra107pre = np.loadtxt('../TAO_raw/temp107a.flg', skiprows=5,
			dtype=temp, converters=cnv)

  Ntime = len(ra107pre['date'])

  weight_pre = np.arange(Ntime-1,-1,-1)/(Ntime-1)
  weight_post = np.arange(0,Ntime)/(Ntime-1)

  window_len = 13
  for depth in ra107['sal-pre'].dtype.names:
      ra107['dens-pre'][depth] = ra107['dens-pre'][depth] + 1000
      ra107['dens-post'][depth] = ra107['dens-post'][depth] + 1000
      ra107['temp'][depth] = ra107pre['temp'][depth]

      # pre to post-cal interpolation
      ra107['sal'][depth] = weight_pre * ra107['sal-pre'][depth] \
                            + weight_post * ra107['sal-post'][depth]
      ra107['dens'][depth] = weight_pre * ra107['dens-pre'][depth] \
                            + weight_post * ra107['dens-post'][depth]

      # filter hourly
      ra107['temp-hr'][depth] = smooth(ra107['temp'][depth], window_len)[0::window_len/2]
      ra107['sal-hr'][depth] = smooth(ra107['sal'][depth], window_len)[0::window_len/2]
      ra107['dens-hr'][depth] = smooth(ra107['dens'][depth], window_len)[0::window_len/2]

  ra107['hr-time'] = ra107['date'][0::window_len/2]

  # read netCDF data
  salfilename='../s12n90e_dy.cdf'
  tempfilename='../t12n90e_dy.cdf'

  import netCDF4 as nc

  salfile = nc.Dataset(salfilename)
  tempfile = nc.Dataset(tempfilename)

  # t0 = np.datetime64(salfile['time'].units[14:])
  t0 = dt.datetime.strptime(salfile['time'].units[11:],
			    '%Y-%m-%d %H:%M:%S')
  timevec = np.array([t0 + dt.timedelta(days=tt.astype('float')) \
			for tt in salfile['time'][0:]])

  ind107start = np.argmin(np.abs(timevec - ra107['date'][0]))
  ind107stop = np.argmin(np.abs(timevec - ra107['date'][-1]))


  tindex = [np.where(tempfile['depth'][:] == zz)[0][0] for zz in salfile['depth'][:]]
  temp_matrix = tempfile['T_20'][ind107start:ind107stop+1].squeeze()
  temp_matrix[temp_matrix > 40] = np.nan
  sal_matrix = salfile['S_41'][ind107start:ind107stop+1].squeeze()
  sal_matrix[sal_matrix > 40] = np.nan

  dens_matrix = sw.pden(sal_matrix, temp_matrix[:,tindex], salfile['depth'][:])
  # save processed salinity product
  ra107['sal-dy'] = dict([])
  ra107['temp-dy'] = dict([])
  ra107['dens-dy'] = dict([])
  ra107['dy-time'] = timevec[ind107start:ind107stop+1]
  for index, zz in enumerate(np.int32(salfile['depth'][:])):
      ra107['sal-dy'][str(zz)] = sal_matrix[:,index]
      ra107['temp-dy'][str(zz)] = temp_matrix[:,tindex[index]]
      ra107['dens-dy'][str(zz)] = dens_matrix[:,index]

#+END_SRC

#+RESULTS:

** Salinity
*** Difference between pre- and post-salinity.

Post-cal salinity is roughly 0.06 psu lower everywhere.
#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-sal-pre-post-cal.png

  plt.plot_date(ra107['date'],
		ra107['sal-pre']['10'] - ra107['sal-post']['10'], '-')
  plt.title('RAMA13 pre-cal salinity - post-cal salinity at 10m')
#+END_SRC

#+RESULTS:
[[file:images/rama13-sal-pre-post-cal.png]]
*** Compare pre- and post-cal
#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-interp-pre-post-sal.png
  depth = '10'
  plt.figure()
  plt.plot(ra107['sal'][depth] - ra107['sal-pre'][depth], label='interp-pre')
  plt.plot(ra107['sal'][depth] - ra107['sal-post'][depth], label='interp-post')
  plt.axhline(0)
  plt.legend()
  plt.title(depth + 'm depth')
#+END_SRC

#+RESULTS:
[[file:images/rama13-interp-pre-post-sal.png]]

Nothing crazy in the interpolated product. Spiky at the surface, perhaps that's expected.

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-interp-salinity.png
  plt.figure()

  for depth in ra107['sal'].dtype.names:
	plt.plot_date(ra107['date'][0:-1:6],
                      ra107['sal'][depth][0:-1:6], '-',
                      label=depth, linewidth=1)

  plt.legend()
  plt.title('ra-107 / RAMA13 interpolated pre-cal post-cal salinity product')

#+END_SRC

#+RESULTS:
[[file:images/rama13-interp-salinity.png]]
** Temperature
*** Read in netCDF 10 minute data                                :noexport:
This is the same as date read from pre-cal .flg file.

There is no post-cal for temperature.

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports none
  tempfilename = '../t12n90e_10m.cdf'

  import netCDF4 as nc

  tempfile = nc.Dataset(tempfilename)

  # t0 = np.datetime64(tempfile['time'].units[14:])
  t0 = dt.datetime.strptime(tempfile['time'].units[14:],
			    '%Y-%m-%d %H:%M:%S')
  timevec = np.array([t0 + dt.timedelta(minutes=tt.astype('float')) \
                      for tt in tempfile['time'][0:]])

  ind107start = np.argmin(np.abs(timevec - ra107['date'][0]))
  ind107stop = np.argmin(np.abs(timevec - ra107['date'][-1]))

  temp_matrix = tempfile['T_20'][ind107start:ind107stop+1].squeeze()

  # save processed temperature product
  for index, zz in enumerate(np.int32(tempfile['depth'][:])):
      ra107['temp-proc'][str(zz)] = temp_matrix[:,index]

  # now for pre-calib temperature
  cnv = {0:ProcessDate}
  for jj in np.arange(1,14):
      cnv[jj] = CleanSalinity;

  ra107pre = np.loadtxt('../TAO_raw/temp107a.flg', skiprows=5, dtype=temp,
			converters=cnv)

  ra107['temp'] = ra107pre['temp']
#+END_SRC

#+RESULTS:
*** Compare fully-processed and "preliminary" data = no difference :noexport:
#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports none :file images/ra107-pre-proc-temp.png
  for index, zz in enumerate(['1', '10', '20', '40']):
      plt.subplot(4,1,index+1)
      plt.plot_date(ra107['date'],
                    ra107['temp-proc'][zz]-ra107['temp'][zz],
                    '-', linewidth=1)
#+END_SRC

#+RESULTS:
[[file:ra107-pre-proc-temp.png]]
** 10 min vs. daily data
*** functions :noexport:
#+BEGIN_SRC ipython :session  :eval never-exports :tangle yes
  def Compare10mDyDiff(rama, var, proc='', filt=False, window_len=13):
      ''' Compares 10m and daily differences of quantities '''
      import matplotlib as mpl
      monthsFmt = mpl.dates.DateFormatter("%d-%m")

      if var is 'sal':
          label = 'S'

      if var is 'temp':
          label = 'T'

      if var is 'dens':
          label = 'ρ'

      if proc is not '' and proc[0] is not '-':
          proc = '-' + proc

      if filt is False:
          window_len = 2

      depths = list(rama[var].keys())
      for index, [d1, d2] in enumerate(zip(depths[0:-3], depths[1:-2])):
          hax = plt.subplot(3,1,index+1)
          dens1 = smooth(rama[var + proc][d1], window_len=window_len)
          dens2 = smooth(rama[var + proc][d2], window_len=window_len)
          plt.plot(rama['date'][0::window_len/2],
                   dens2[0::window_len/2]-dens1[0::window_len/2], linewidth=1)
          plt.plot(rama['dy-time'],
                   rama[var + '-dy'][d2] - rama[var + '-dy'][d1], linewidth=1)
          plt.axhline(0, color='k')
          if var is 'sal':
              plt.axhline(0.06, color='gray')
              plt.axhline(-0.06, color='gray')

          plt.ylabel('Δ' + label + ' ' + d2 + 'm-' + d1 + 'm')
          plt.ylim(limy)
          hax.xaxis.set_major_formatter(monthsFmt)

      plt.gcf().suptitle(proc)
      plt.show()

  def Compare10mDy(rama, var, proc=''):
      ''' Plots 10min and daily timeseries of var'''
      if var is 'sal':
          label = 'S'

      if var is 'temp':
          label = 'T'

      if var is 'dens':
          label = 'ρ'

      if proc is not '' and proc[0] is not '-':
          proc = '-' + proc

      for index,zz in enumerate(['1', '10', '20', '40']):
          plt.subplot(4,1,index+1)
          datenum = mpl.dates.date2num(ra107['date'])
          plt.plot(datenum, ra107[var + proc][zz], linewidth=1)
          plt.ylabel(label + ' ' + zz + 'm')
          plt.plot(ra107['dy-time'], ra107[var + '-dy'][zz], linewidth=1)

            # if index == 0:
            #     mask = ra107['N2'][0,:] < 0

            # if index == 1:
            #     mask = np.logical_or(ra107['N2'][0,:] < 0,
            #                          ra107['N2'][1,:] < 0)

            # if index == 1:
            #     mask = ra107['N2'][1,:] < 0

            # plt.plot(datenum[mask], ra107[var][zz][mask],
            # 'r.', markersize=2)

            # plt.xlim([735260, 735280])

      plt.gcf().suptitle(proc)
      plt.show()
#+END_SRC

#+RESULTS:
*** Quality flags for daily data
Wut, nothing's been flagged as adjusted in the daily data? ¯\ _(ツ)_/¯

Quality flag = 2 = default quality; Pre-deployment calibrations applied.  Default value for sensors presently deployed and for sensors which were either not recovered or not calibratable when recovered.

*The daily data are using pre-calibration coefficients*

Looks like they depend on the Hanning filter to deal with spiking.

#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-quality-dy.png

  plt.subplot(211)
  plt.plot_date(ra107['dy-time'],
		salfile['QS_5041'][ind107start:ind107stop+1,1:4].squeeze() , '-', linewidth=1)
  plt.title('Sal')

  plt.subplot(212)
  plt.plot_date(ra107['dy-time'],
		tempfile['QT_5020'][ind107start:ind107stop+1,1:4].squeeze() , '-', linewidth=1)
  plt.title('Temp')
  plt.show()
#+END_SRC

#+RESULTS:
[[file:images/rama13-quality-dy.png]]

*** Salinity
#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-sal-10m-dy.png
Compare10mDy(ra107, 'sal')
#+END_SRC

#+RESULTS:
[[file:images/rama13-sal-10m-dy.png]]


#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-sal-diff-10m-dy.png
Compare10mDyDiff(ra107, 'sal', '')
#+END_SRC

#+RESULTS:
[[file:images/rama13-sal-diff-10m-dy.png]]

*** Temperature
#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-temp-10m-dy.png
Compare10mDy(ra107, 'temp')
#+END_SRC

#+RESULTS:
[[file:images/rama13-temp-10m-dy.png]]


#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-temp-diff-10m-dy.png
Compare10mDyDiff(ra107, 'temp')
#+END_SRC

#+RESULTS:
[[file:images/rama13-temp-diff-10m-dy.png]]

*** *Potential* Density
Using density from .flg files is consistent. There seem be a lot of density inversions between 10m and 20m depths; especially at the beginning of the record.

winter convection?

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-dens-10m-dy.png
 Compare10mDy(ra107, 'dens')
#+END_SRC

#+RESULTS:
[[file:images/rama13-dens-10m-dy.png]]


#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-dens-diff-10m-dy.png
Compare10mDyDiff(ra107, 'dens', '', filt=True, window_len=13)
#+END_SRC

#+RESULTS:
[[file:images/rama13-dens-diff-10m-dy.png]]

** Density inversions
*** How many exist in /10 minute/ data
The percentage of valid data with N² < 0
#+BEGIN_SRC ipython :session :eval never-export :exports results
  def TabulateNegativeN2(p_ave, N2, dSdz, dTdz):
      ''' Percentage of valid data that yields N² < 0 '''
      table = [list(p_ave[:,0]),
	       [np.round(len(n[n<0])/len(n)*100) for n in # % N² < 0
		[N2[i,~np.isnan(N2[i,:])] for i in range(N2.shape[0])]],
	       [np.round(len(s[s>0])/len(s)*100) for s in # % dS/dz > 0
		[dSdz[i,~np.isnan(dSdz[i,:])] for i in range(dSdz.shape[0])]],
	       [np.round(len(s[s<0])/len(s)*100) for s in # % dT/dz > 0
		[dTdz[i,~np.isnan(dTdz[i,:])] for i in range(dTdz.shape[0])]]]

      table[0].insert(0, 'Depth (m)')
      table[1].insert(0, '% N² < 0')
      table[2].insert(0, '% dS/dz > 0')
      table[3].insert(0, '% dT/dz < 0')

      return table

  def MakeArrays(rama, proc=''):
      rama['salarr'] = np.array([rama['sal' + proc]['1'],
			          rama['sal' + proc]['10'],
			          rama['sal' + proc]['20'],
			          rama['sal' + proc]['40'],
			          rama['sal' + proc]['60'],
			          rama['sal' + proc]['100']])

      rama['temparr'] = np.array([rama['temp' + proc]['1'],
			           rama['temp' + proc]['10'],
			           rama['temp' + proc]['20'],
			           rama['temp' + proc]['40'],
			           rama['temp' + proc]['60'],
			           rama['temp' + proc]['100']])

      rama['densarr'] = np.array([rama['dens' + proc]['1'],
			           rama['dens' + proc]['10'],
			           rama['dens' + proc]['20'],
			           rama['dens' + proc]['40'],
			           rama['dens' + proc]['60'],
			           rama['dens' + proc]['100']])

      rama['presarr'] = np.array([1*np.ones(rama['salarr'][0,:].shape),
			           10*np.ones(rama['salarr'][0,:].shape),
			           20*np.ones(rama['salarr'][0,:].shape),
			           40*np.ones(rama['salarr'][0,:].shape),
			           60*np.ones(rama['salarr'][0,:].shape),
			           100*np.ones(rama['salarr'][0,:].shape)])
      return rama

  def CalcGradients(rama):
      dSdz = -np.diff(rama['salarr'], axis=0)/np.diff(rama['presarr'], axis=0)
      dTdz = -np.diff(rama['temparr'], axis=0)/np.diff(rama['presarr'], axis=0)

      N2,_,p_ave = sw.bfrq(rama['salarr'], rama['temparr'], rama['presarr'], 12)
      rama['N2'] = N2
      return (dSdz, dTdz, N2, p_ave)

  MakeArrays(ra107)
  dSdz, dTdz, N2, p_ave = CalcGradients(ra107)
  table = TabulateNegativeN2(p_ave, N2, dSdz, dTdz)
  table
#+END_SRC

#+RESULTS:
| Depth (m)   |  5.5 | 15.0 | 30.0 | 50.0 | 80.0 |
| % N² < 0    | 61.0 | 12.0 | 21.0 |  0.0 |  0.0 |
| % dS/dz > 0 | 53.0 | 19.0 | 28.0 |  1.0 | 15.0 |
| % dT/dz < 0 | 69.0 | 47.0 | 23.0 |  7.0 |  0.0 |
*** How many exist in /filtered hourly/ data
  #+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export
    MakeArrays(ra107, '-hr')
    dSdz, dTdz, N2, p_ave = CalcGradients(ra107)
    table = TabulateNegativeN2(p_ave, N2, dSdz, dTdz)
    table
  #+END_SRC

  #+RESULTS:
  | Depth (m)   |  5.5 | 15.0 | 30.0 | 50.0 | 80.0 |
  | % N² < 0    | 61.0 | 12.0 | 20.0 |  0.0 |  0.0 |
  | % dS/dz > 0 | 54.0 | 19.0 | 27.0 |  1.0 | 15.0 |
  | % dT/dz < 0 | 69.0 | 48.0 | 22.0 |  7.0 |  0.0 |

*** Where do these occur?
#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-dens-inversion-zoom.png
  tend = 500;
  monthsFmt = mpl.dates.DateFormatter("%d-%m")

  plt.plot(ra107['date'][0:tend],
           ra107['dens']['20'][0:tend] - ra107['dens']['10'][0:tend], linewidth=1)
  plt.axhline(0, color='k')
  plt.ylabel('Δρ 20m-10m')
  hax.xaxis.set_major_formatter(monthsFmt)

#+END_SRC

#+RESULTS:
[[file:images/rama13-dens-inversion-zoom.png]]

*** Funny density offset/trends appear to result from salinity.

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-sal-diff.png
  N2 = np.zeros([2, len(ra107['sal']['10'])])
  N2[0,:] = -9.81/1028 * (ra107['sal']['10']-ra107['sal']['20'])/10
  N2[1,:] = -9.81/1028 * (ra107['sal']['20']-ra107['sal']['40'])/20

  limy = [-0.2, 0.4]

  tend = 500;
  monthsFmt = mpl.dates.DateFormatter("%d-%m")

  depths = list(ra107['sal'].keys())
  for index, [d1, d2] in enumerate(zip(depths[0:-3], depths[1:-2])):
      hax = plt.subplot(3,1,index+1)
      plt.plot(ra107['date'],
               ra107['sal'][d2] - ra107['sal'][d1], linewidth=1)
      plt.plot(ra107['dy-time'],
               ra107['sal-dy'][d2] - ra107['sal-dy'][d1], linewidth=1)
      plt.axhline(0, color='k')
      plt.ylabel('ΔS ' + d2 + 'm-' + d1 + 'm')
      plt.ylim(limy)
      hax.xaxis.set_major_formatter(monthsFmt)

  plt.show()
#+END_SRC

#+RESULTS:
[[file:images/rama13-sal-diff.png]]
*** Contributors to negative N²
Salinity appears to be the controlling factor generally.

#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-dρdz.png

  class MidpointNormalize(mpl.colors.Normalize):
      def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
          self.midpoint = midpoint
          mpl.colors.Normalize.__init__(self, vmin, vmax, clip)

      def __call__(self, value, clip=None):
          # I'm ignoring masked values and all kinds of edge cases to make a
          # simple example...
          x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
          return np.ma.masked_array(np.interp(value, x, y))

  tindex = np.arange(0,dSdz.shape[1])
  # ra107['N2'][ra107['N2'] > 0.05] = np.nan;

  hax = plt.subplot(311)
  plt.pcolormesh(tindex, -ra107['presarr'],
		 1e6*-7.6e-5*np.ma.masked_array(dSdz, np.isnan(dSdz)),
		 norm=MidpointNormalize(midpoint=0.),
		 cmap=cmo.cm.balance)
  plt.title('β dS/dz * 1e6')
  plt.clim(-3, 12)
  plt.colorbar(extend='min')

  hax = plt.subplot(312)
  plt.pcolormesh(tindex, -ra107['presarr'],
		 1e6*-1.7e-4*np.ma.masked_array(dTdz, np.isnan(dTdz)),
		 norm=MidpointNormalize(midpoint=0.),
		 cmap=cmo.cm.balance)
  plt.colorbar(extend='min')
  plt.clim(-3, 12)
  plt.title('-α dT/dz * 1e6')

  hax = plt.subplot(313)
  mycmap = copy(cmo.cm.ice_r)
  mycmap.set_bad(color='w')
  mycmap.set_under(color='r')
  mynorm = mpl.colors.Normalize(vmin=0., vmax=np.nanmax(ra107['N2']))

  plt.pcolormesh(tindex, -ra107['presarr'],
		 np.ma.masked_array(ra107['N2'], np.isnan(ra107['N2'])),
		 cmap=mycmap, norm=mynorm)
  plt.axhline(-15, color='k'); plt.axhline(-30, color='k')
  plt.colorbar(extend='min')
  plt.title('N² (negative in red)')

  plt.tight_layout()
  plt.show()

#+END_SRC

#+RESULTS:
[[file:images/rama13-dρdz.png]]

Let's try a better way.

Looks like both dT/dz < 0, dS/dz > 0 (colder, saltier water on top) are responsible.

Though most points have dS/dz > 0.

#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export :file images/rama13-neg-N²-scatter.png

  for ii in [1,2]:
      plt.subplot(1,2,ii)
      mask = N2[ii,:] < 0
      plt.hexbin(7.6e-1*dSdz[ii,mask], 1.7*dTdz[ii,mask], mincnt=10)
      plt.axis('square')
      plt.axhline(0, color='k', alpha=0.5);
      plt.axvline(0, color='k', alpha=0.5)
      if ii is 1:
          plt.xlim([-0.005, 0.005]); plt.ylim([-0.005, 0.005])
      if ii is 2:
          plt.xlim([-0.0025, 0.0025]); plt.ylim([-0.0025, 0.0025])

      plt.xlabel('β dS/dz * 1e4'); plt.ylabel('α dT/dz * 1e4')
      plt.title(str(p_ave[ii,0]) + 'm')

  plt.gcf().suptitle('N² < 0 points binned', y=0.75)
  plt.tight_layout()
  plt.show()
#+END_SRC

#+RESULTS:
[[file:images/rama13-neg-N²-scatter.png]]
** Save data

#+BEGIN_SRC ipython :session :tangle yes :exports results :eval never-export
  def SaveRama(rama, proc=''):
      ''' This saves a (depth, time) matrix of temp, sal, pres to
      RamaPrelimProcessed/rama['name'].mat '''

      from scipy.io import savemat

      def datetime2matlabdn(dt):
          import datetime as date
          ord = dt.toordinal()
          mdn = dt + date.timedelta(days = 366)
          frac = (dt-date.datetime(dt.year,dt.month,dt.day,0,0,0)).seconds \
		 / (24.0 * 60.0 * 60.0)
          return mdn.toordinal() + frac

      MakeArrays(rama, proc)

      if proc is '':
          datevec = rama['date']
      else:
          if proc[0] is '-':
              proc = proc[1:]

          datevec = rama[proc + '-time']

      datenum = np.array([datetime2matlabdn(dd) for dd in datevec])
      mdict = {'time' : datenum,
	       'sal' : rama['salarr'],
	       'temp' : rama['temparr'],
	       'depth' : rama['presarr'][:,0]}

      savemat('../RamaPrelimProcessed/' + rama['name'], mdict, do_compression=True)

  SaveRama(ra107)
#+END_SRC

#+RESULTS:

* RAMA14 (ra-122)

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results
  ra122read = np.loadtxt('../TAO_raw/sal122a.flg', skiprows=5, dtype=sal,
			 converters={0:ProcessDate,
			             1:CleanSalinity,
			             2:CleanSalinity,
			             3:CleanSalinity,
			             4:CleanSalinity,
			             5:CleanSalinity,
			             6:CleanSalinity})

  ra122 = dict([])
  ra122['date'] = ra122read['date']
  ra122['sal']  = ra122read['sal']
  ra122['temp'] = dict([])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama14-pre-cal-salinity.png
    plt.figure()

    for depth in ra122['sal'].dtype.names:
	  plt.plot_date(ra122['date'][0:-1:6],
			ra122['sal'][depth][0:-1:6], '-',
			label=depth, linewidth=1)

    plt.legend()
    plt.title('ra-122 / RAMA14 pre-cal salinity product')
#+END_SRC

#+RESULTS:
[[file:images/rama14-pre-cal-salinity.png]]

* Full record
** What are the differences between end of RAMA13 and start of RAMA14

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results
  ramadiff = np.dtype([('depth', np.int32),
                       ('ΔS', np.float32),
                       ('Δt', dt.timedelta)])

  diff = np.zeros((6,), dtype=ramadiff)

  for index,depth in enumerate(ra107['sal'].keys()):
      r13 = ra107['sal'][depth]
      sal13 = r13[~np.isnan(r13)]
      date13 = ra107['date'][~np.isnan(r13)]

      diff[index] = (int(depth),
                     ra122['sal'][depth][0] - r13[-1],
                     ra122['date'][0] - date13[-1])

  diff
#+END_SRC

#+RESULTS:
: array([(  1,         nan, datetime.timedelta(27, 61200)),
:        ( 10,  0.02700043, datetime.timedelta(0, 46200)),
:        ( 20,  0.01599884, datetime.timedelta(0, 46200)),
:        ( 40,  0.47800064, datetime.timedelta(0, 46200)),
:        ( 60,  0.0359993 , datetime.timedelta(0, 46200)),
:        (100,  0.00300217, datetime.timedelta(0, 46200))],
:       dtype=[('depth', '<i4'), ('ΔS', '<f4'), ('Δt', 'O')])

(depth, ΔS, Δtime)

ra107 surface instrument failed a month before recovery.

The rest seem OK except for the 40m instrument: during recovery/deployment there is a big jump of 0.5 psu.

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/ra07-ra122-switch-period.png
  for index, depth in enumerate(ra107['sal'].keys()):
      if index == 0:
          continue

      hax = plt.subplot(6,1,index+1)

      plt.plot_date(ra107['date'][-100:-1],
	            ra107['sal'][depth][-100:-1],
	            'k*-', linewidth=1)
      plt.plot_date(ra122['date'][0:100],
	            ra122['sal'][depth][0:100],
	            'k*-', linewidth=1)

      if index < 5:
          hax.set_xticklabels([], visible=False)

      plt.title(depth+'m')

  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:images/ra07-ra122-switch-period.png]]

** Plot full record - 10 min salinity

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-rama14-full-salinity.png

  dtime = 1
  for index, depth in enumerate(ra107['sal'].keys()):
       hax = plt.subplot(6,1,index+1)
       rama = ra107
       plt.plot_date(rama['date'][0:-1:dtime],
	             rama['sal'][depth][0:-1:dtime], 'k-',
	             label=depth, linewidth=1)

       rama = ra122
       plt.plot_date(rama['date'][0:-1:dtime],
	             rama['sal'][depth][0:-1:dtime], 'k-',
	             label=depth, linewidth=1)
       plt.title(depth + 'm')
       if index == 0:
           plt.title('RAMA 13 & 14 salinity | 1m')

       plt.ylim([31.5, 35.5])
       if index < 5:
            hax.set_xticklabels(labels=[], visible=False)

  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:images/rama13-rama14-full-salinity.png]]

40m and 60m  instruments seem to be a lot noisier!

Emily thinks this is because of the thermocline being sloshed up and down by internal waves.

let's check distribution / variances - variances are only slightly higher.

#+BEGIN_SRC ipython :session :tangle yes :eval never-export :exports results :file images/rama13-rama14-sal-histograms.png
  def dcHist(var, bins=100, **kwargs):
    import numpy as np
    mpl.rcParams['figure.facecolor'] = 'None'
    plt.hist(var[~np.isnan(var)], bins,
             normed=True, alpha=0.7, **kwargs)

  for index, depth in enumerate(ra107['sal'].dtype.names):
    plt.subplot(3,2,index+1)
    dcHist(ra107['sal'][depth], label='13/107')
    dcHist(ra122['sal'][depth], label='14/122')
    plt.title(depth + 'm | var = '
              + str(np.nanvar(ra107['sal'][depth]))[0:5]
              + ' | var = '
              + str(np.nanvar(ra122['sal'][depth]))[0:5])
    if index == 0:
      plt.legend()

  plt.suptitle('Normalized histogram for 10min salinity', va='bottom')
  plt.tight_layout()

#+END_SRC

#+RESULTS:
[[file:images/rama13-rama14-sal-histograms.png]]
